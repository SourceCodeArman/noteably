<rpg-method>
# Repository Planning Graph (RPG) Method - PRD Template

This template teaches you (AI or human) how to create structured, dependency-aware PRDs using the RPG methodology from Microsoft Research. The key insight: separate WHAT (functional) from HOW (structural), then connect them with explicit dependencies.

## Core Principles

1. **Dual-Semantics**: Think functional (capabilities) AND structural (code organization) separately, then map them
2. **Explicit Dependencies**: Never assume - always state what depends on what
3. **Topological Order**: Build foundation first, then layers on top
4. **Progressive Refinement**: Start broad, refine iteratively

## How to Use This Template

- Follow the instructions in each `<instruction>` block
- Look at `<example>` blocks to see good vs bad patterns
- Fill in the content sections with your project details
- The AI reading this will learn the RPG method by following along
- Task Master will parse the resulting PRD into dependency-aware tasks

## Recommended Tools for Creating PRDs

When using this template to **create** a PRD (not parse it), use **code-context-aware AI assistants** for best results:

**Why?** The AI needs to understand your existing codebase to make good architectural decisions about modules, dependencies, and integration points.

**Recommended tools:**
- **Claude Code** (claude-code CLI) - Best for structured reasoning and large contexts
- **Cursor/Windsurf** - IDE integration with full codebase context
- **Gemini CLI** (gemini-cli) - Massive context window for large codebases
- **Codex/Grok CLI** - Strong code generation with context awareness

**Note:** Once your PRD is created, `task-master parse-prd` works with any configured AI model - it just needs to read the PRD text itself, not your codebase.
</rpg-method>

---

<overview>
<instruction>
Start with the problem, not the solution. Be specific about:
- What pain point exists?
- Who experiences it?
- Why existing solutions don't work?
- What success looks like (measurable outcomes)?

Keep this section focused - don't jump into implementation details yet.
</instruction>

## Problem Statement

Students, professionals, and lifelong learners struggle to efficiently convert educational content from diverse sources (YouTube videos, lectures, documents) into structured study materials. The current workflow is fragmented: users must manually transcribe audio/video, extract key information, create summaries, generate flashcards, and build quizzes - a time-consuming process that takes hours for content that could be processed in minutes.

Existing solutions fail because:
- **Manual transcription tools** (Otter.ai, Rev) only provide raw text, requiring additional manual work to create study materials
- **Note-taking apps** (Notion, Obsidian) lack AI-powered content generation from multimedia sources
- **Flashcard apps** (Anki, Quizlet) require manual card creation, defeating the purpose of efficiency
- **No unified platform** handles the full pipeline from source ingestion to structured output

The core pain point: **There is no "upload and study" solution** that automatically transforms any input format (video, audio, document) into ready-to-use study materials (summaries, notes, flashcards, quizzes).

## Target Users

### Primary Persona: College Student (Sarah)
- **Workflow**: Attends lectures, watches YouTube tutorials, reads PDFs for multiple courses
- **Pain**: Spends 3-4 hours per day creating study materials from recorded lectures and videos
- **Goal**: Reduce study prep time by 70% while maintaining quality
- **Tech comfort**: High - uses multiple apps daily

### Secondary Persona: Professional Learner (Marcus)
- **Workflow**: Consumes online courses, webinars, technical documentation for career development
- **Pain**: Struggles to retain information from long-form content without structured review materials
- **Goal**: Create spaced repetition study materials from professional development content
- **Tech comfort**: Medium-High - prefers simple, efficient tools

### Tertiary Persona: Educator (Dr. Chen)
- **Workflow**: Creates study materials for students from lecture recordings and course materials
- **Pain**: Time-consuming to generate diverse study materials (summaries, quizzes) for different learning styles
- **Goal**: Rapidly generate multiple study formats from single source material
- **Tech comfort**: Medium - needs reliable, straightforward tools

## Success Metrics

**User Engagement:**
- 80% of users complete at least one full pipeline (upload → transcription → generation → export) within first session
- Average 5+ content items processed per user per month
- 70% of users return within 7 days

**Quality Metrics:**
- < 5% transcription error rate (measured against ground truth for test set)
- 85% user satisfaction rating for generated content quality
- < 10% content regeneration rate (users satisfied with first output)

**Performance Metrics:**
- < 30 seconds processing time for files under 5 minutes
- < 2 minutes processing time for files under 30 minutes
- 99% uptime for transcription and generation services

**Business Metrics:**
- Average cost per user per month: < $2.50 (AI provider costs)
- Feature usage distribution: 40% summaries, 30% notes, 20% flashcards, 10% quizzes

</overview>

---

<functional-decomposition>
<instruction>
Now think about CAPABILITIES (what the system DOES), not code structure yet.

Step 1: Identify high-level capability domains
- Think: "What major things does this system do?"
- Examples: Data Management, Core Processing, Presentation Layer

Step 2: For each capability, enumerate specific features
- Use explore-exploit strategy:
  * Exploit: What features are REQUIRED for core value?
  * Explore: What features make this domain COMPLETE?

Step 3: For each feature, define:
- Description: What it does in one sentence
- Inputs: What data/context it needs
- Outputs: What it produces/returns
- Behavior: Key logic or transformations

<example type="good">
Capability: Data Validation
  Feature: Schema validation
    - Description: Validate JSON payloads against defined schemas
    - Inputs: JSON object, schema definition
    - Outputs: Validation result (pass/fail) + error details
    - Behavior: Iterate fields, check types, enforce constraints

  Feature: Business rule validation
    - Description: Apply domain-specific validation rules
    - Inputs: Validated data object, rule set
    - Outputs: Boolean + list of violated rules
    - Behavior: Execute rules sequentially, short-circuit on failure
</example>

<example type="bad">
Capability: validation.js
  (Problem: This is a FILE, not a CAPABILITY. Mixing structure into functional thinking.)

Capability: Validation
  Feature: Make sure data is good
  (Problem: Too vague. No inputs/outputs. Not actionable.)
</example>
</instruction>

## Capability Tree

### Capability: Source Ingestion
Handles accepting and validating content from multiple input sources (YouTube URLs, file uploads, text paste).

#### Feature: YouTube URL Processing
- **Description**: Extract video metadata and download audio/video content from YouTube URLs
- **Inputs**: YouTube URL string, user authentication token
- **Outputs**: Video metadata (title, duration, description, thumbnail), downloaded media file reference
- **Behavior**: Validate URL format, use YouTube Data API for metadata, download via yt-dlp or similar, store reference in system

#### Feature: File Upload Handling
- **Description**: Accept and validate uploaded files (audio, video, PDF, DOCX, TXT) with size and type checks
- **Inputs**: Multipart file upload, file metadata (name, size, type)
- **Outputs**: File storage reference, validation result, extracted metadata
- **Behavior**: Validate file type against whitelist, check size limits, extract MIME type, store in object storage, return storage key

#### Feature: Text Input Processing
- **Description**: Accept plain text paste input and normalize formatting
- **Inputs**: Raw text string, optional metadata (title, source)
- **Outputs**: Normalized text content, character count, detected language
- **Behavior**: Strip extra whitespace, detect encoding, perform basic normalization, detect language

#### Feature: Metadata Extraction
- **Description**: Extract structured metadata from various input sources (titles, headings, timestamps, speaker info)
- **Inputs**: Source content (file or URL), content type
- **Outputs**: Structured metadata object (title, duration, headings, speaker labels if available)
- **Behavior**: Parse file headers, extract PDF headings, parse video metadata, detect document structure

### Capability: Transcription Pipeline
Converts audio/video content into text with timestamps and optional speaker separation.

#### Feature: Provider Selection
- **Description**: Choose transcription provider (Whisper API or AssemblyAI) based on content characteristics and user preferences
- **Inputs**: Content type, file duration, user preferences, provider availability
- **Outputs**: Selected provider identifier, estimated cost, estimated processing time
- **Behavior**: Evaluate file characteristics (duration, language, quality), check provider status, apply selection rules (cost vs quality trade-offs)

#### Feature: Audio/Video Transcription
- **Description**: Transcribe audio/video content to text with word-level timestamps
- **Inputs**: Media file reference, language hint, provider selection
- **Outputs**: Transcription text, word-level timestamps, confidence scores, detected language
- **Behavior**: Upload media to provider, initiate transcription job, poll for completion, retrieve results, parse timestamp data

#### Feature: Speaker Separation
- **Description**: Identify and label different speakers in transcribed content (when supported by provider)
- **Inputs**: Transcription result, audio file reference
- **Outputs**: Speaker-labeled segments, speaker count, speaker identification confidence
- **Behavior**: Request speaker diarization from provider, match speakers across segments, assign consistent labels

#### Feature: Language Detection
- **Description**: Automatically detect the primary language of content for transcription
- **Inputs**: Audio file reference or text sample
- **Outputs**: Detected language code, confidence score
- **Behavior**: Use provider's language detection API or analyze audio characteristics, return ISO 639-1 language code

#### Feature: Transcription Error Handling
- **Description**: Handle transcription failures, timeouts, and retries with exponential backoff
- **Inputs**: Error response from provider, attempt count, file characteristics
- **Outputs**: Retry decision, error message for user, fallback provider selection
- **Behavior**: Classify error type (timeout, rate limit, invalid file), determine retry strategy, switch providers if needed, log error details

### Capability: Content Generation Pipeline
Generates structured study materials (summaries, notes, flashcards, quizzes) from transcribed or input text using LLM providers.

#### Feature: LLM Provider Abstraction
- **Description**: Provide unified interface for multiple LLM providers (Gemini, Anthropic, OpenAI, AssemblyAI LeMUR)
- **Inputs**: Generation request, provider selection, model parameters
- **Outputs**: Standardized response format, provider-specific metadata
- **Behavior**: Normalize request format, route to appropriate provider SDK, handle provider-specific error formats, normalize response structure

#### Feature: Summary Generation
- **Description**: Generate concise summaries in three length variants (short, medium, long) from source content
- **Inputs**: Source text, summary length preference, optional focus areas
- **Outputs**: Summary text (short/medium/long variants), key points list, topic tags
- **Behavior**: Construct prompt with length specification, call LLM with temperature settings, extract and structure summary, validate length requirements

#### Feature: Detailed Notes Generation
- **Description**: Generate structured, hierarchical notes with headings, bullet points, and key concepts
- **Inputs**: Source text, note style preference (outline, Cornell, mind-map structure)
- **Outputs**: Structured notes (markdown format), concept hierarchy, important definitions
- **Behavior**: Analyze content structure, identify key concepts, generate hierarchical outline, format as markdown with proper headings

#### Feature: Flashcard Generation
- **Description**: Generate Q/A pairs suitable for spaced repetition learning
- **Inputs**: Source text, number of flashcards desired, difficulty level
- **Outputs**: Array of flashcard objects (question, answer, tags, difficulty), total count
- **Behavior**: Extract key concepts, formulate questions covering different cognitive levels (recall, application, analysis), generate concise answers, tag by topic

#### Feature: Quiz Generation
- **Description**: Generate multiple choice and short answer questions with answer keys
- **Inputs**: Source text, quiz type (multiple choice, short answer, mixed), number of questions
- **Outputs**: Quiz object (questions array, answer key, explanations), question metadata
- **Behavior**: Identify testable concepts, generate distractors for multiple choice, create answer explanations, ensure question variety and difficulty distribution

#### Feature: Content Refinement
- **Description**: Allow users to regenerate or refine generated content with additional instructions
- **Inputs**: Original generation, refinement instructions, content type
- **Outputs**: Refined content matching new specifications
- **Behavior**: Combine original prompt with refinement instructions, maintain context from original generation, apply changes while preserving quality

### Capability: User Interface & Interactions
Provides web-based interface for uploading content, viewing status, and interacting with generated materials.

#### Feature: Upload/Import Interface
- **Description**: Provide drag-and-drop and file picker interface for content ingestion
- **Inputs**: User interactions (drag, click, paste), file objects
- **Outputs**: Upload progress, validation feedback, file preview
- **Behavior**: Handle drag-and-drop events, validate files client-side, show upload progress, preview file metadata

#### Feature: Real-time Status Updates
- **Description**: Display live progress updates for transcription and generation processes via WebSockets
- **Inputs**: Process status events from backend, user session
- **Outputs**: Status messages, progress percentages, estimated time remaining
- **Behavior**: Establish WebSocket connection, receive status updates, update UI in real-time, handle connection failures gracefully

#### Feature: Output Viewer
- **Description**: Display generated content in tabbed interface (Summary, Notes, Flashcards, Quiz)
- **Inputs**: Generated content objects, user selection (tab)
- **Outputs**: Rendered content view, formatted appropriately for content type
- **Behavior**: Parse markdown/structured content, render flashcards with flip animation, render quiz with interactive questions, handle tab switching

#### Feature: Export Functionality
- **Description**: Export generated content to PDF, Markdown, or HTML formats
- **Inputs**: Content type, selected content, export format preference
- **Outputs**: Downloadable file (PDF/MD/HTML)
- **Behavior**: Format content for target format, generate PDF with proper styling, create markdown file, generate HTML with embedded styles, trigger browser download

#### Feature: Content History Management
- **Description**: Save and retrieve previously generated content for user
- **Inputs**: User session, content objects, search/filter criteria
- **Outputs**: List of saved content items, individual content retrieval
- **Behavior**: Store content with metadata, implement search/filter, paginate results, allow deletion

#### Feature: Regeneration Interface
- **Description**: Provide UI controls to regenerate or refine generated content
- **Inputs**: User refinement instructions, original content reference
- **Outputs**: Updated content, regeneration status
- **Behavior**: Capture user input, send refinement request, show loading state, update content display

### Capability: Authentication & Authorization
Manages user accounts, authentication, and access control.

#### Feature: User Registration
- **Description**: Create new user accounts with email verification
- **Inputs**: Email, password, optional name
- **Outputs**: User account object, verification token
- **Behavior**: Validate email format, hash password, create user record, send verification email, generate JWT token

#### Feature: User Login
- **Description**: Authenticate users and issue JWT tokens
- **Inputs**: Email, password
- **Outputs**: JWT access token, refresh token, user profile
- **Behavior**: Verify credentials, check account status, generate JWT tokens, return user data

#### Feature: Token Management
- **Description**: Handle JWT token refresh and validation
- **Inputs**: Refresh token, access token
- **Outputs**: New access token, token validity status
- **Behavior**: Validate refresh token, issue new access token, handle token expiration, revoke tokens on logout

#### Feature: Access Control
- **Description**: Enforce user-specific access to content and resources
- **Inputs**: User identity, resource identifier, action type
- **Outputs**: Authorization decision (allow/deny)
- **Behavior**: Check user ownership of resources, validate permissions, enforce rate limits, log access attempts

### Capability: Storage & File Management
Manages file storage in cloud object storage and database references.

#### Feature: Object Storage Integration
- **Description**: Store and retrieve files from R2 or S3-compatible storage
- **Inputs**: File data, storage key, operation type (upload/download/delete)
- **Outputs**: Storage URL, operation success status
- **Behavior**: Upload files with proper content-type headers, generate unique storage keys, handle storage errors, implement retry logic

#### Feature: File Reference Management
- **Description**: Track file metadata and references in database
- **Inputs**: File upload result, user ID, file metadata
- **Outputs**: File record with storage reference
- **Behavior**: Create database record, link to user, store storage key, track file lifecycle

#### Feature: File Cleanup
- **Description**: Remove orphaned or expired files from storage
- **Inputs**: File references, expiration criteria
- **Outputs**: Cleanup report (files deleted, errors)
- **Behavior**: Identify files not referenced in database, check expiration dates, delete from storage, update database records

### Capability: Background Processing
Manages asynchronous task processing for transcription and generation.

#### Feature: Task Queue Management
- **Description**: Queue and manage background tasks using Celery
- **Inputs**: Task type, task parameters, priority
- **Outputs**: Task ID, queue position, estimated start time
- **Behavior**: Create Celery task, assign to appropriate queue, set priority, return task identifier

#### Feature: Transcription Task Execution
- **Description**: Execute transcription asynchronously with progress tracking
- **Inputs**: File reference, transcription parameters, task ID
- **Outputs**: Transcription result, progress updates, completion status
- **Behavior**: Upload file to provider, poll for completion, emit progress events, store results, handle errors

#### Feature: Generation Task Execution
- **Description**: Execute content generation asynchronously with progress tracking
- **Inputs**: Source text, generation type, parameters, task ID
- **Outputs**: Generated content, progress updates, completion status
- **Behavior**: Call LLM provider, stream responses, emit progress, store results, handle rate limits

#### Feature: Task Status Tracking
- **Description**: Track and report status of background tasks
- **Inputs**: Task ID, status updates
- **Outputs**: Current task status, progress percentage, error messages
- **Behavior**: Update task state in database, emit WebSocket events, calculate progress, store error details

### Capability: Analytics & Metrics
Tracks usage, performance, and business metrics.

#### Feature: Usage Tracking
- **Description**: Record user actions and feature usage
- **Inputs**: User ID, action type, metadata, timestamp
- **Outputs**: Analytics event record
- **Behavior**: Log events to analytics database, anonymize sensitive data, batch events for efficiency, track feature usage patterns

#### Feature: Performance Metrics Collection
- **Description**: Collect timing and performance data for operations
- **Inputs**: Operation type, duration, success/failure, metadata
- **Outputs**: Performance metric record
- **Behavior**: Measure operation duration, record success rates, track error frequencies, aggregate by operation type

#### Feature: Cost Tracking
- **Description**: Track AI provider costs per user and operation
- **Inputs**: Provider name, operation type, token usage, cost
- **Outputs**: Cost record with attribution
- **Behavior**: Calculate costs from provider responses, attribute to users, aggregate by time period, generate cost reports

#### Feature: Analytics Dashboard
- **Description**: Provide aggregated analytics views for administrators
- **Inputs**: Time range, metric filters, aggregation level
- **Outputs**: Aggregated metrics, charts data, trend analysis
- **Behavior**: Query analytics database, aggregate by time periods, calculate trends, format for visualization

### Capability: Error Handling & Resilience
Provides robust error handling, retry logic, and system resilience.

#### Feature: Error Classification
- **Description**: Categorize errors by type and severity for appropriate handling
- **Inputs**: Error object, context, operation type
- **Outputs**: Error classification, severity level, handling strategy
- **Behavior**: Analyze error codes, classify by type (transient, permanent, user error), assign severity, determine retry strategy

#### Feature: Retry Logic
- **Description**: Implement exponential backoff retry for transient failures
- **Inputs**: Failed operation, error type, attempt count
- **Outputs**: Retry decision, delay duration, max attempts reached
- **Behavior**: Check error classification, calculate exponential backoff delay, enforce max attempts, log retry attempts

#### Feature: Fallback Mechanisms
- **Description**: Provide fallback options when primary providers fail
- **Inputs**: Failed provider, operation type, available alternatives
- **Outputs**: Fallback provider selection, fallback result
- **Behavior**: Identify alternative providers, switch to fallback, maintain operation continuity, log fallback usage

#### Feature: Error Reporting
- **Description**: Log and report errors for monitoring and debugging
- **Inputs**: Error object, context, user ID, operation details
- **Outputs**: Error log entry, alert notification (if critical)
- **Behavior**: Format error details, store in error log, send alerts for critical errors, include context for debugging

</functional-decomposition>

---

<structural-decomposition>
<instruction>
NOW think about code organization. Map capabilities to actual file/folder structure.

Rules:
1. Each capability maps to a module (folder or file)
2. Features within a capability map to functions/classes
3. Use clear module boundaries - each module has ONE responsibility
4. Define what each module exports (public interface)

The goal: Create a clear mapping between "what it does" (functional) and "where it lives" (structural).

<example type="good">
Capability: Data Validation
  → Maps to: src/validation/
    ├── schema-validator.js      (Schema validation feature)
    ├── rule-validator.js         (Business rule validation feature)
    └── index.js                  (Public exports)

Exports:
  - validateSchema(data, schema)
  - validateRules(data, rules)
</example>

<example type="bad">
Capability: Data Validation
  → Maps to: src/utils.js
  (Problem: "utils" is not a clear module boundary. Where do I find validation logic?)

Capability: Data Validation
  → Maps to: src/validation/everything.js
  (Problem: One giant file. Features should map to separate files for maintainability.)
</example>
</instruction>

## Repository Structure

```
noteably/
├── backend/
│   ├── config/
│   │   ├── settings.py
│   │   ├── urls.py
│   │   └── wsgi.py
│   ├── apps/
│   │   ├── ingestion/              # Source Ingestion capability
│   │   │   ├── models.py
│   │   │   ├── serializers.py
│   │   │   ├── views.py
│   │   │   ├── youtube_handler.py
│   │   │   ├── file_handler.py
│   │   │   ├── text_handler.py
│   │   │   └── metadata_extractor.py
│   │   ├── transcription/         # Transcription Pipeline capability
│   │   │   ├── models.py
│   │   │   ├── serializers.py
│   │   │   ├── views.py
│   │   │   ├── provider_selector.py
│   │   │   ├── whisper_client.py
│   │   │   ├── assemblyai_client.py
│   │   │   ├── speaker_separation.py
│   │   │   ├── language_detector.py
│   │   │   └── error_handler.py
│   │   ├── generation/             # Content Generation Pipeline capability
│   │   │   ├── models.py
│   │   │   ├── serializers.py
│   │   │   ├── views.py
│   │   │   ├── llm_provider.py
│   │   │   ├── gemini_client.py
│   │   │   ├── anthropic_client.py
│   │   │   ├── openai_client.py
│   │   │   ├── assemblyai_lemur_client.py
│   │   │   ├── summary_generator.py
│   │   │   ├── notes_generator.py
│   │   │   ├── flashcard_generator.py
│   │   │   ├── quiz_generator.py
│   │   │   └── refinement_handler.py
│   │   ├── accounts/               # Authentication & Authorization capability
│   │   │   ├── models.py
│   │   │   ├── serializers.py
│   │   │   ├── views.py
│   │   │   ├── registration.py
│   │   │   ├── login.py
│   │   │   ├── token_manager.py
│   │   │   └── permissions.py
│   │   ├── storage/                # Storage & File Management capability
│   │   │   ├── models.py
│   │   │   ├── object_storage.py
│   │   │   ├── file_manager.py
│   │   │   └── cleanup.py
│   │   ├── tasks/                  # Background Processing capability
│   │   │   ├── celery_app.py
│   │   │   ├── transcription_tasks.py
│   │   │   ├── generation_tasks.py
│   │   │   └── status_tracker.py
│   │   ├── analytics/             # Analytics & Metrics capability
│   │   │   ├── models.py
│   │   │   ├── usage_tracker.py
│   │   │   ├── performance_collector.py
│   │   │   ├── cost_tracker.py
│   │   │   └── dashboard.py
│   │   └── core/                   # Shared utilities
│   │       ├── exceptions.py
│   │       ├── error_handler.py
│   │       ├── retry_logic.py
│   │       └── fallback.py
│   ├── tasks/                      # Celery tasks directory
│   └── manage.py
├── frontend/
│   ├── src/
│   │   ├── components/
│   │   │   ├── UploadInterface/    # Upload/Import Interface feature
│   │   │   ├── StatusDisplay/      # Real-time Status Updates feature
│   │   │   ├── OutputViewer/       # Output Viewer feature
│   │   │   ├── ExportButton/       # Export Functionality feature
│   │   │   ├── ContentHistory/     # Content History Management feature
│   │   │   └── RegenerationPanel/  # Regeneration Interface feature
│   │   ├── services/
│   │   │   ├── api.js
│   │   │   ├── websocket.js
│   │   │   └── export.js
│   │   ├── pages/
│   │   │   ├── Login.js
│   │   │   ├── Register.js
│   │   │   ├── Dashboard.js
│   │   │   └── History.js
│   │   └── App.js
│   └── package.json
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
└── docs/
```

## Module Definitions

### Module: ingestion
- **Maps to capability**: Source Ingestion
- **Responsibility**: Handle content ingestion from multiple sources (YouTube, files, text)
- **File structure**:
  ```
  ingestion/
  ├── models.py              # Django models for ingestion records
  ├── serializers.py         # DRF serializers
  ├── views.py               # API endpoints
  ├── youtube_handler.py     # YouTube URL Processing feature
  ├── file_handler.py        # File Upload Handling feature
  ├── text_handler.py        # Text Input Processing feature
  └── metadata_extractor.py  # Metadata Extraction feature
  ```
- **Exports**:
  - `process_youtube_url(url)` - Extract metadata and download from YouTube
  - `handle_file_upload(file)` - Validate and store uploaded file
  - `process_text_input(text)` - Normalize and process text input
  - `extract_metadata(source)` - Extract structured metadata

### Module: transcription
- **Maps to capability**: Transcription Pipeline
- **Responsibility**: Convert audio/video to text with timestamps and speaker separation
- **File structure**:
  ```
  transcription/
  ├── models.py              # Transcription records, status
  ├── serializers.py         # DRF serializers
  ├── views.py               # API endpoints
  ├── provider_selector.py   # Provider Selection feature
  ├── whisper_client.py      # Whisper API integration
  ├── assemblyai_client.py   # AssemblyAI integration
  ├── speaker_separation.py  # Speaker Separation feature
  ├── language_detector.py  # Language Detection feature
  └── error_handler.py       # Transcription Error Handling feature
  ```
- **Exports**:
  - `select_provider(content_type, duration)` - Choose transcription provider
  - `transcribe_audio(file_ref, provider)` - Transcribe audio/video
  - `detect_speakers(transcription)` - Identify speaker segments
  - `detect_language(content)` - Detect content language
  - `handle_transcription_error(error, context)` - Error handling and retry

### Module: generation
- **Maps to capability**: Content Generation Pipeline
- **Responsibility**: Generate study materials using LLM providers
- **File structure**:
  ```
  generation/
  ├── models.py              # Generated content records
  ├── serializers.py         # DRF serializers
  ├── views.py               # API endpoints
  ├── llm_provider.py        # LLM Provider Abstraction feature
  ├── gemini_client.py       # Google Gemini integration
  ├── anthropic_client.py    # Anthropic Claude integration
  ├── openai_client.py       # OpenAI integration
  ├── assemblyai_lemur_client.py  # AssemblyAI LeMUR integration
  ├── summary_generator.py   # Summary Generation feature
  ├── notes_generator.py     # Detailed Notes Generation feature
  ├── flashcard_generator.py # Flashcard Generation feature
  ├── quiz_generator.py      # Quiz Generation feature
  └── refinement_handler.py  # Content Refinement feature
  ```
- **Exports**:
  - `LLMProvider` - Abstract base class for LLM providers
  - `generate_summary(text, length)` - Generate summaries
  - `generate_notes(text, style)` - Generate structured notes
  - `generate_flashcards(text, count)` - Generate Q/A flashcards
  - `generate_quiz(text, type, count)` - Generate quiz questions
  - `refine_content(content, instructions)` - Refine generated content

### Module: accounts
- **Maps to capability**: Authentication & Authorization
- **Responsibility**: User management, authentication, and access control
- **File structure**:
  ```
  accounts/
  ├── models.py              # User model
  ├── serializers.py         # DRF serializers
  ├── views.py               # API endpoints
  ├── registration.py       # User Registration feature
  ├── login.py               # User Login feature
  ├── token_manager.py       # Token Management feature
  └── permissions.py         # Access Control feature
  ```
- **Exports**:
  - `register_user(email, password)` - Create new user account
  - `login_user(email, password)` - Authenticate and issue tokens
  - `refresh_token(refresh_token)` - Refresh access token
  - `check_permission(user, resource, action)` - Authorization check

### Module: storage
- **Maps to capability**: Storage & File Management
- **Responsibility**: Manage file storage in cloud object storage
- **File structure**:
  ```
  storage/
  ├── models.py              # File reference models
  ├── object_storage.py      # Object Storage Integration feature
  ├── file_manager.py        # File Reference Management feature
  └── cleanup.py             # File Cleanup feature
  ```
- **Exports**:
  - `upload_file(file_data, key)` - Upload to object storage
  - `download_file(storage_key)` - Download from storage
  - `delete_file(storage_key)` - Delete from storage
  - `create_file_reference(file_data, user_id)` - Create DB record
  - `cleanup_orphaned_files()` - Remove orphaned files

### Module: tasks
- **Maps to capability**: Background Processing
- **Responsibility**: Manage asynchronous task processing with Celery
- **File structure**:
  ```
  tasks/
  ├── celery_app.py          # Celery configuration
  ├── transcription_tasks.py # Transcription Task Execution feature
  ├── generation_tasks.py    # Generation Task Execution feature
  └── status_tracker.py      # Task Status Tracking feature
  ```
- **Exports**:
  - `queue_transcription_task(file_ref, params)` - Queue transcription
  - `queue_generation_task(source_text, type, params)` - Queue generation
  - `get_task_status(task_id)` - Get current task status
  - `cancel_task(task_id)` - Cancel running task

### Module: analytics
- **Maps to capability**: Analytics & Metrics
- **Responsibility**: Track usage, performance, and costs
- **File structure**:
  ```
  analytics/
  ├── models.py              # Analytics event models
  ├── usage_tracker.py       # Usage Tracking feature
  ├── performance_collector.py  # Performance Metrics Collection feature
  ├── cost_tracker.py        # Cost Tracking feature
  └── dashboard.py           # Analytics Dashboard feature
  ```
- **Exports**:
  - `track_event(user_id, action, metadata)` - Record user action
  - `record_performance(operation, duration, success)` - Record metrics
  - `track_cost(provider, operation, cost)` - Record AI costs
  - `get_analytics(time_range, filters)` - Get aggregated metrics

### Module: core
- **Maps to capability**: Error Handling & Resilience
- **Responsibility**: Shared error handling, retry logic, and resilience patterns
- **File structure**:
  ```
  core/
  ├── exceptions.py          # Custom exception classes
  ├── error_handler.py       # Error Classification feature
  ├── retry_logic.py         # Retry Logic feature
  └── fallback.py            # Fallback Mechanisms feature
  ```
- **Exports**:
  - `classify_error(error, context)` - Classify error type and severity
  - `retry_with_backoff(func, max_attempts)` - Retry decorator
  - `get_fallback_provider(provider, operation)` - Get fallback option
  - `log_error(error, context)` - Error reporting

</structural-decomposition>

---

<dependency-graph>
<instruction>
This is THE CRITICAL SECTION for Task Master parsing.

Define explicit dependencies between modules. This creates the topological order for task execution.

Rules:
1. List modules in dependency order (foundation first)
2. For each module, state what it depends on
3. Foundation modules should have NO dependencies
4. Every non-foundation module should depend on at least one other module
5. Think: "What must EXIST before I can build this module?"

<example type="good">
Foundation Layer (no dependencies):
  - error-handling: No dependencies
  - config-manager: No dependencies
  - base-types: No dependencies

Data Layer:
  - schema-validator: Depends on [base-types, error-handling]
  - data-ingestion: Depends on [schema-validator, config-manager]

Core Layer:
  - algorithm-engine: Depends on [base-types, error-handling]
  - pipeline-orchestrator: Depends on [algorithm-engine, data-ingestion]
</example>

<example type="bad">
- validation: Depends on API
- API: Depends on validation
(Problem: Circular dependency. This will cause build/runtime issues.)

- user-auth: Depends on everything
(Problem: Too many dependencies. Should be more focused.)
</example>
</instruction>

## Dependency Chain

### Foundation Layer (Phase 0)
No dependencies - these are built first.

- **core**: Provides error handling, retry logic, and fallback mechanisms used by all other modules
- **accounts**: Provides authentication and authorization - required before any user-facing features

### Infrastructure Layer (Phase 1)
Depends on foundation modules.

- **storage**: Depends on [core, accounts]
  - Uses core for error handling
  - Uses accounts for user association with files

### Data Ingestion Layer (Phase 2)
Depends on infrastructure and foundation.

- **ingestion**: Depends on [storage, core, accounts]
  - Uses storage for file uploads
  - Uses core for error handling
  - Uses accounts for user authentication

### Processing Layer (Phase 3)
Depends on ingestion and infrastructure.

- **transcription**: Depends on [ingestion, storage, core]
  - Uses ingestion for source content
  - Uses storage for file access
  - Uses core for error handling and retry logic

- **generation**: Depends on [transcription, core]
  - Uses transcription for source text
  - Uses core for error handling and provider fallback

### Orchestration Layer (Phase 4)
Depends on processing modules.

- **tasks**: Depends on [transcription, generation, core]
  - Orchestrates transcription and generation tasks
  - Uses core for error handling

### Analytics Layer (Phase 5)
Depends on all functional modules.

- **analytics**: Depends on [accounts, ingestion, transcription, generation, tasks, core]
  - Tracks events from all modules
  - Uses core for error handling

</dependency-graph>

---

<implementation-roadmap>
<instruction>
Turn the dependency graph into concrete development phases.

Each phase should:
1. Have clear entry criteria (what must exist before starting)
2. Contain tasks that can be parallelized (no inter-dependencies within phase)
3. Have clear exit criteria (how do we know phase is complete?)
4. Build toward something USABLE (not just infrastructure)

Phase ordering follows topological sort of dependency graph.

<example type="good">
Phase 0: Foundation
  Entry: Clean repository
  Tasks:
    - Implement error handling utilities
    - Create base type definitions
    - Setup configuration system
  Exit: Other modules can import foundation without errors

Phase 1: Data Layer
  Entry: Phase 0 complete
  Tasks:
    - Implement schema validator (uses: base types, error handling)
    - Build data ingestion pipeline (uses: validator, config)
  Exit: End-to-end data flow from input to validated output
</example>

<example type="bad">
Phase 1: Build Everything
  Tasks:
    - API
    - Database
    - UI
    - Tests
  (Problem: No clear focus. Too broad. Dependencies not considered.)
</example>
</instruction>

## Development Phases

### Phase 0: Foundation & Authentication
**Goal**: Establish core error handling infrastructure and user authentication system

**Entry Criteria**: 
- Django project initialized
- Database configured
- Python virtual environment set up

**Tasks**:
- [ ] Implement core error handling module (depends on: none)
  - Acceptance criteria: Custom exception classes defined, error classification function works, error logging implemented
  - Test strategy: Unit tests for error classification, exception handling, logging

- [ ] Implement retry logic with exponential backoff (depends on: none)
  - Acceptance criteria: Retry decorator works, exponential backoff calculated correctly, max attempts enforced
  - Test strategy: Unit tests for retry behavior, backoff calculation, failure scenarios

- [ ] Implement fallback mechanism framework (depends on: none)
  - Acceptance criteria: Fallback provider selection works, fallback chain execution implemented
  - Test strategy: Unit tests for fallback selection, chain execution

- [ ] Setup Django user model and authentication (depends on: none)
  - Acceptance criteria: User model created, registration endpoint works, login endpoint issues JWT tokens
  - Test strategy: Integration tests for registration, login, token validation

- [ ] Implement JWT token management (depends on: accounts)
  - Acceptance criteria: Token generation works, refresh endpoint functional, token validation middleware works
  - Test strategy: Integration tests for token lifecycle, refresh flow

**Exit Criteria**: 
- All modules can import core utilities without errors
- Users can register, login, and receive JWT tokens
- Error handling works across test scenarios

**Delivers**: 
- Developers can use error handling and retry utilities in other modules
- Users can authenticate and access protected endpoints

---

### Phase 1: Storage Infrastructure
**Goal**: Establish file storage system for uploaded content

**Entry Criteria**: Phase 0 complete

**Tasks**:
- [ ] Setup object storage client (R2/S3) (depends on: core)
  - Acceptance criteria: Can upload files, download files, delete files, handles storage errors gracefully
  - Test strategy: Integration tests with mock storage, error handling tests

- [ ] Implement file reference models (depends on: accounts, core)
  - Acceptance criteria: File model created, links to users, stores metadata, tracks lifecycle
  - Test strategy: Unit tests for model relationships, database operations

- [ ] Implement file upload API endpoint (depends on: storage, accounts)
  - Acceptance criteria: Accepts multipart uploads, validates file types, stores in object storage, creates DB record
  - Test strategy: Integration tests for upload flow, validation, error cases

- [ ] Implement file cleanup service (depends on: storage)
  - Acceptance criteria: Identifies orphaned files, deletes from storage, updates database
  - Test strategy: Unit tests for cleanup logic, integration tests with test files

**Exit Criteria**: 
- Files can be uploaded, stored, and retrieved
- File references are tracked in database
- Cleanup service works correctly

**Delivers**: 
- System can store user-uploaded files
- Files are properly tracked and can be cleaned up

---

### Phase 2: Source Ingestion
**Goal**: Enable content ingestion from multiple sources (YouTube, files, text)

**Entry Criteria**: Phase 1 complete

**Tasks**:
- [ ] Implement YouTube URL handler (depends on: storage, core)
  - Acceptance criteria: Validates YouTube URLs, extracts metadata, downloads audio/video, stores file reference
  - Test strategy: Integration tests with YouTube API, mock tests for error cases

- [ ] Implement file upload handler (depends on: storage, core)
  - Acceptance criteria: Validates file types, checks size limits, extracts MIME type, stores file
  - Test strategy: Unit tests for validation logic, integration tests for upload flow

- [ ] Implement text input processor (depends on: core)
  - Acceptance criteria: Normalizes text, detects language, extracts basic metadata
  - Test strategy: Unit tests for normalization, language detection

- [ ] Implement metadata extractor (depends on: ingestion)
  - Acceptance criteria: Extracts titles, headings, duration, speaker info from various sources
  - Test strategy: Unit tests for each source type, integration tests with real files

- [ ] Create ingestion API endpoints (depends on: ingestion, accounts)
  - Acceptance criteria: Accepts YouTube URLs, file uploads, text input, returns source references
  - Test strategy: Integration tests for all input types, error handling

**Exit Criteria**: 
- Users can upload files, paste YouTube URLs, or input text
- All sources are validated and stored correctly
- Metadata is extracted and stored

**Delivers**: 
- Users can input content from multiple sources
- System has source content ready for processing

---

### Phase 3: Transcription Pipeline
**Goal**: Convert audio/video content to text with timestamps

**Entry Criteria**: Phase 2 complete

**Tasks**:
- [ ] Implement provider selector (depends on: core)
  - Acceptance criteria: Selects provider based on content characteristics, returns selection with cost estimate
  - Test strategy: Unit tests for selection logic, different content scenarios

- [ ] Implement Whisper API client (depends on: core)
  - Acceptance criteria: Uploads audio, initiates transcription, polls for completion, retrieves results with timestamps
  - Test strategy: Integration tests with Whisper API, mock tests for error cases

- [ ] Implement AssemblyAI client (depends on: core)
  - Acceptance criteria: Uploads audio, initiates transcription, handles speaker diarization, retrieves results
  - Test strategy: Integration tests with AssemblyAI API, speaker separation tests

- [ ] Implement language detection (depends on: transcription)
  - Acceptance criteria: Detects language from audio or text, returns ISO code with confidence
  - Test strategy: Unit tests with known languages, accuracy validation

- [ ] Implement transcription error handling (depends on: core, transcription)
  - Acceptance criteria: Handles timeouts, rate limits, invalid files, implements retry with fallback
  - Test strategy: Unit tests for error classification, integration tests for retry flow

- [ ] Create transcription API endpoints (depends on: transcription, ingestion)
  - Acceptance criteria: Accepts source reference, initiates transcription, returns task ID, provides status endpoint
  - Test strategy: Integration tests for transcription flow, status updates

**Exit Criteria**: 
- Audio/video files can be transcribed with timestamps
- Speaker separation works when available
- Error handling and retries work correctly
- Transcription status can be queried

**Delivers**: 
- Users can transcribe audio/video content
- System provides text output ready for content generation

---

### Phase 4: Content Generation Pipeline
**Goal**: Generate study materials (summaries, notes, flashcards, quizzes) from text

**Entry Criteria**: Phase 3 complete

**Tasks**:
- [ ] Implement LLM provider abstraction (depends on: core)
  - Acceptance criteria: Base provider interface defined, unified request/response format, provider switching works
  - Test strategy: Unit tests for abstraction, mock provider implementations

- [ ] Implement Gemini client (depends on: generation)
  - Acceptance criteria: Connects to Gemini API, formats requests, parses responses, handles errors
  - Test strategy: Integration tests with Gemini API, error handling tests

- [ ] Implement Anthropic client (depends on: generation)
  - Acceptance criteria: Connects to Anthropic API, formats requests, parses responses, handles errors
  - Test strategy: Integration tests with Anthropic API, error handling tests

- [ ] Implement OpenAI client (depends on: generation)
  - Acceptance criteria: Connects to OpenAI API, formats requests, parses responses, handles errors
  - Test strategy: Integration tests with OpenAI API, error handling tests

- [ ] Implement AssemblyAI LeMUR client (depends on: generation)
  - Acceptance criteria: Connects to LeMUR API, formats requests, parses responses, handles errors
  - Test strategy: Integration tests with LeMUR API, error handling tests

- [ ] Implement summary generator (depends on: generation)
  - Acceptance criteria: Generates short/medium/long summaries, extracts key points, returns structured output
  - Test strategy: Unit tests for prompt construction, integration tests with LLM providers

- [ ] Implement notes generator (depends on: generation)
  - Acceptance criteria: Generates structured notes with headings, supports different styles, returns markdown
  - Test strategy: Unit tests for structure parsing, integration tests with LLM providers

- [ ] Implement flashcard generator (depends on: generation)
  - Acceptance criteria: Generates Q/A pairs, covers different cognitive levels, returns structured flashcards
  - Test strategy: Unit tests for question generation logic, integration tests with LLM providers

- [ ] Implement quiz generator (depends on: generation)
  - Acceptance criteria: Generates multiple choice and short answer questions, creates answer keys, includes explanations
  - Test strategy: Unit tests for question types, integration tests with LLM providers

- [ ] Implement content refinement handler (depends on: generation)
  - Acceptance criteria: Accepts refinement instructions, regenerates content, maintains quality
  - Test strategy: Integration tests for refinement flow, quality validation

- [ ] Create generation API endpoints (depends on: generation, transcription)
  - Acceptance criteria: Accepts source text and type, initiates generation, returns task ID, provides status endpoint
  - Test strategy: Integration tests for generation flow, all content types

**Exit Criteria**: 
- All content types can be generated from text input
- Multiple LLM providers are supported with fallback
- Generated content is structured and usable
- Refinement works correctly

**Delivers**: 
- Users can generate summaries, notes, flashcards, and quizzes
- System produces high-quality study materials

---

### Phase 5: Background Processing
**Goal**: Enable asynchronous processing of transcription and generation tasks

**Entry Criteria**: Phase 4 complete

**Tasks**:
- [ ] Setup Celery with Redis/RabbitMQ (depends on: core)
  - Acceptance criteria: Celery workers running, tasks can be queued, results can be retrieved
  - Test strategy: Integration tests for task queue, worker execution

- [ ] Implement transcription task (depends on: tasks, transcription)
  - Acceptance criteria: Queues transcription, executes asynchronously, emits progress updates, stores results
  - Test strategy: Integration tests for async execution, progress tracking

- [ ] Implement generation task (depends on: tasks, generation)
  - Acceptance criteria: Queues generation, executes asynchronously, emits progress updates, stores results
  - Test strategy: Integration tests for async execution, progress tracking

- [ ] Implement task status tracking (depends on: tasks, core)
  - Acceptance criteria: Tracks task state, calculates progress, stores errors, provides status API
  - Test strategy: Unit tests for status tracking, integration tests for status API

- [ ] Implement WebSocket support for real-time updates (depends on: tasks)
  - Acceptance criteria: WebSocket connection established, status updates pushed, handles disconnections
  - Test strategy: Integration tests for WebSocket flow, reconnection handling

**Exit Criteria**: 
- Transcription and generation tasks run asynchronously
- Task status can be tracked in real-time
- WebSocket updates work correctly
- Long-running tasks don't block API

**Delivers**: 
- Users can submit tasks and receive real-time updates
- System handles long-running operations efficiently

---

### Phase 6: User Interface
**Goal**: Provide web interface for uploading content and viewing results

**Entry Criteria**: Phase 5 complete

**Tasks**:
- [ ] Implement upload/import interface component (depends on: ingestion API)
  - Acceptance criteria: Drag-and-drop works, file picker works, shows upload progress, validates files client-side
  - Test strategy: E2E tests for upload flow, unit tests for validation

- [ ] Implement real-time status display component (depends on: WebSocket)
  - Acceptance criteria: Shows progress updates, displays status messages, handles reconnection
  - Test strategy: E2E tests for status updates, unit tests for WebSocket handling

- [ ] Implement output viewer component (depends on: generation API)
  - Acceptance criteria: Displays all content types, tabbed interface works, renders markdown, shows flashcards and quizzes
  - Test strategy: E2E tests for viewing, unit tests for rendering

- [ ] Implement export functionality (depends on: generation API)
  - Acceptance criteria: Exports to PDF, Markdown, HTML, triggers download, formats correctly
  - Test strategy: E2E tests for export, unit tests for formatting

- [ ] Implement content history component (depends on: generation API, accounts)
  - Acceptance criteria: Lists saved content, search/filter works, pagination works, can delete items
  - Test strategy: E2E tests for history, unit tests for filtering

- [ ] Implement regeneration interface component (depends on: generation API)
  - Acceptance criteria: Accepts refinement instructions, triggers regeneration, updates display
  - Test strategy: E2E tests for regeneration, unit tests for input handling

- [ ] Create authentication pages (login/register) (depends on: accounts API)
  - Acceptance criteria: Login form works, registration form works, error handling, redirects correctly
  - Test strategy: E2E tests for auth flow, unit tests for form validation

- [ ] Create main dashboard page (depends on: all UI components)
  - Acceptance criteria: Integrates all components, navigation works, responsive design
  - Test strategy: E2E tests for full workflow, responsive design tests

**Exit Criteria**: 
- Users can upload content through UI
- Real-time status updates display correctly
- Generated content displays in all formats
- Export functionality works
- History and regeneration work
- Full user workflow is functional

**Delivers**: 
- Users can use the complete application through web interface
- All features are accessible and functional

---

### Phase 7: Analytics & Monitoring
**Goal**: Track usage, performance, and costs

**Entry Criteria**: Phase 6 complete

**Tasks**:
- [ ] Implement usage tracking (depends on: analytics, all modules)
  - Acceptance criteria: Tracks user actions, records feature usage, stores events in database
  - Test strategy: Integration tests for event tracking, unit tests for event formatting

- [ ] Implement performance metrics collection (depends on: analytics, core)
  - Acceptance criteria: Measures operation duration, records success rates, tracks errors
  - Test strategy: Unit tests for metric collection, integration tests for timing

- [ ] Implement cost tracking (depends on: analytics, generation, transcription)
  - Acceptance criteria: Tracks AI provider costs, attributes to users, calculates totals
  - Test strategy: Unit tests for cost calculation, integration tests for cost tracking

- [ ] Implement analytics dashboard API (depends on: analytics)
  - Acceptance criteria: Provides aggregated metrics, supports filtering, returns chart data
  - Test strategy: Integration tests for aggregation, unit tests for filtering

- [ ] Create admin analytics dashboard UI (depends on: analytics API)
  - Acceptance criteria: Displays metrics, shows charts, supports filtering, updates in real-time
  - Test strategy: E2E tests for dashboard, unit tests for data visualization

**Exit Criteria**: 
- Usage events are tracked for all features
- Performance metrics are collected
- Costs are tracked accurately
- Analytics dashboard displays data correctly

**Delivers**: 
- Administrators can monitor system usage and performance
- Business metrics are available for decision-making

</implementation-roadmap>

---

<test-strategy>
<instruction>
Define how testing will be integrated throughout development (TDD approach).

Specify:
1. Test pyramid ratios (unit vs integration vs e2e)
2. Coverage requirements
3. Critical test scenarios
4. Test generation guidelines for Surgical Test Generator

This section guides the AI when generating tests during the RED phase of TDD.

<example type="good">
Critical Test Scenarios for Data Validation module:
  - Happy path: Valid data passes all checks
  - Edge cases: Empty strings, null values, boundary numbers
  - Error cases: Invalid types, missing required fields
  - Integration: Validator works with ingestion pipeline
</example>
</instruction>

## Test Pyramid

```
        /\
       /E2E\       ← 10% (End-to-end user workflows, critical paths)
      /------\
     /Integration\ ← 30% (Module interactions, API endpoints, external services)
    /------------\
   /  Unit Tests  \ ← 60% (Fast, isolated, deterministic, all business logic)
  /----------------\
```

## Coverage Requirements
- Line coverage: 80% minimum
- Branch coverage: 75% minimum
- Function coverage: 85% minimum
- Statement coverage: 80% minimum

## Critical Test Scenarios

### ingestion Module
**Happy path**:
- Upload valid PDF file → File stored, metadata extracted, reference returned
- Paste YouTube URL → Metadata extracted, video downloaded, reference returned
- Paste text input → Text normalized, language detected, reference returned
- Expected: All input types processed successfully, metadata stored correctly

**Edge cases**:
- Upload file at size limit boundary → Handles correctly, validates size
- YouTube URL with special characters → URL parsed correctly
- Empty text input → Handles gracefully, returns appropriate error
- Expected: System handles boundary conditions without crashing

**Error cases**:
- Invalid file type uploaded → Returns validation error, file rejected
- Invalid YouTube URL → Returns error, no download attempted
- Corrupted file uploaded → Detects corruption, returns error
- Expected: Clear error messages, no partial state, proper cleanup

**Integration points**:
- File upload → Storage → Database record creation
- YouTube URL → Download → Storage → Database record
- Expected: End-to-end flow works, all components integrated correctly

### transcription Module
**Happy path**:
- Transcribe short audio file (< 5 min) → Transcription completed, timestamps included
- Transcribe with speaker separation → Speakers identified, segments labeled
- Expected: Accurate transcription, proper formatting, timestamps correct

**Edge cases**:
- Very long audio file (> 2 hours) → Handles chunking, completes successfully
- Low quality audio → Provider handles gracefully, returns best-effort transcription
- Multiple languages in one file → Detects primary language, handles mixed content
- Expected: System handles edge cases, provides reasonable results

**Error cases**:
- Transcription provider timeout → Retries with backoff, switches provider if needed
- Invalid audio file → Detects early, returns clear error
- Provider API error → Handles gracefully, implements fallback
- Expected: Errors handled, retries work, fallback activates

**Integration points**:
- Ingestion → Transcription → Storage of results
- Transcription → Error handling → Retry logic
- Expected: Full pipeline works, error handling integrated

### generation Module
**Happy path**:
- Generate summary from text → Summary created, length matches request, key points extracted
- Generate flashcards → Q/A pairs created, appropriate difficulty, properly formatted
- Generate quiz → Questions created, answer key included, explanations provided
- Expected: High-quality output, proper structure, meets requirements

**Edge cases**:
- Very short input text → Handles gracefully, generates appropriate content
- Very long input text → Handles chunking, maintains context
- Multiple content types requested → All generated correctly, stored separately
- Expected: System handles various input sizes, produces quality output

**Error cases**:
- LLM provider rate limit → Implements backoff, switches provider
- LLM provider timeout → Retries, handles gracefully
- Invalid generation parameters → Validates early, returns clear error
- Expected: Provider errors handled, retries work, fallback activates

**Integration points**:
- Transcription → Generation → Storage of results
- Generation → Refinement → Updated content
- Expected: Full pipeline works, refinement maintains quality

### accounts Module
**Happy path**:
- User registration → Account created, verification email sent, can login
- User login → JWT tokens issued, tokens valid, refresh works
- Expected: Authentication flow works, tokens secure

**Edge cases**:
- Registration with existing email → Returns appropriate error
- Login with invalid credentials → Returns error, no token issued
- Token refresh near expiration → New token issued, old token invalidated
- Expected: Edge cases handled correctly, security maintained

**Error cases**:
- Invalid JWT token → Rejects request, returns 401
- Expired refresh token → Requires re-login
- Rate limiting on auth endpoints → Enforces limits, returns 429
- Expected: Security enforced, errors handled properly

**Integration points**:
- Registration → Email service → Database
- Login → Token generation → Protected endpoint access
- Expected: Full auth flow works, protected endpoints secured

### tasks Module
**Happy path**:
- Queue transcription task → Task queued, executes asynchronously, status updates sent
- Queue generation task → Task queued, executes asynchronously, results stored
- Expected: Tasks execute correctly, status tracking works

**Edge cases**:
- Multiple tasks queued simultaneously → All execute, no conflicts
- Task cancellation → Task stops, cleanup performed, status updated
- Expected: Concurrent tasks handled, cancellation works

**Error cases**:
- Task execution failure → Error stored, status updated, user notified
- Worker crash during task → Task retried, no data loss
- Expected: Failures handled, retries work, data integrity maintained

**Integration points**:
- API → Task queue → Worker → Status updates → WebSocket
- Expected: Full async flow works, real-time updates delivered

## Test Generation Guidelines

### Unit Tests
- **Focus**: Business logic, data transformations, validation
- **Pattern**: Arrange-Act-Assert
- **Mocking**: Mock external APIs, database operations, file system
- **Naming**: `test_<function>_<scenario>_<expected_result>`
- **Example**: `test_validate_file_type_valid_pdf_returns_true()`

### Integration Tests
- **Focus**: Module interactions, API endpoints, database operations
- **Pattern**: Setup test data → Execute operation → Verify results → Cleanup
- **Mocking**: Mock external APIs (YouTube, LLM providers), use test database
- **Naming**: `test_<module>_<operation>_<scenario>`
- **Example**: `test_ingestion_upload_file_creates_database_record()`

### E2E Tests
- **Focus**: Complete user workflows, critical paths
- **Pattern**: User action → System response → Verify outcome
- **Mocking**: Mock external APIs, use test environment
- **Naming**: `test_e2e_<user_workflow>`
- **Example**: `test_e2e_upload_youtube_generate_summary_export_pdf()`

### Test Data Management
- Use factories for test data creation (factory_boy for Django)
- Create reusable fixtures for common scenarios
- Clean up test data after each test
- Use test-specific configuration (separate database, storage bucket)

### External Service Mocking
- Mock all external API calls (YouTube, Whisper, AssemblyAI, LLM providers)
- Use responses library for HTTP mocking
- Create mock responses that match real API formats
- Test both success and error responses

### Performance Testing
- Test transcription with various file sizes
- Test generation with various text lengths
- Measure API response times
- Test concurrent request handling

### Security Testing
- Test authentication and authorization
- Test input validation and sanitization
- Test rate limiting
- Test JWT token handling

</test-strategy>

---

<architecture>
<instruction>
Describe technical architecture, data models, and key design decisions.

Keep this section AFTER functional/structural decomposition - implementation details come after understanding structure.
</instruction>

## System Components

### Backend Architecture
- **Django REST Framework**: Main web framework for API endpoints
- **Celery**: Distributed task queue for asynchronous processing
- **Redis/RabbitMQ**: Message broker for Celery tasks
- **PostgreSQL**: Primary database for structured data
- **R2/S3-compatible Storage**: Object storage for files
- **WebSocket (Django Channels)**: Real-time status updates

### Frontend Architecture
- **React**: UI framework for interactive components
- **WebSocket Client**: Real-time status updates
- **File Upload Library**: Drag-and-drop and file picker
- **Markdown Renderer**: Display generated notes and summaries
- **PDF Export Library**: Client-side PDF generation

### External Services
- **YouTube Data API**: Video metadata extraction
- **OpenAI Whisper API**: Transcription service option 1
- **AssemblyAI**: Transcription service option 2 (with speaker diarization)
- **Google Gemini**: LLM provider option 1
- **Anthropic Claude**: LLM provider option 2
- **OpenAI GPT**: LLM provider option 3
- **AssemblyAI LeMUR**: LLM provider option 4

## Data Models

### User Model
```python
User:
  - id (UUID, primary key)
  - email (string, unique, indexed)
  - password_hash (string)
  - name (string, optional)
  - created_at (datetime)
  - email_verified (boolean)
  - is_active (boolean)
```

### Source Model
```python
Source:
  - id (UUID, primary key)
  - user (ForeignKey to User)
  - source_type (enum: youtube, file, text)
  - source_reference (string)  # URL, storage key, or text hash
  - metadata (JSONField)  # Title, duration, headings, etc.
  - created_at (datetime)
  - status (enum: pending, processing, completed, failed)
```

### File Model
```python
File:
  - id (UUID, primary key)
  - user (ForeignKey to User)
  - storage_key (string, indexed)
  - original_filename (string)
  - file_type (enum: audio, video, document)
  - mime_type (string)
  - size_bytes (integer)
  - uploaded_at (datetime)
  - expires_at (datetime, optional)
```

### Transcription Model
```python
Transcription:
  - id (UUID, primary key)
  - source (ForeignKey to Source)
  - provider (enum: whisper, assemblyai)
  - text (text)
  - timestamps (JSONField)  # Word-level timestamps
  - speakers (JSONField, optional)  # Speaker segments
  - language (string)  # ISO 639-1 code
  - confidence_scores (JSONField, optional)
  - created_at (datetime)
  - processing_time_seconds (float)
```

### GeneratedContent Model
```python
GeneratedContent:
  - id (UUID, primary key)
  - source (ForeignKey to Source)
  - transcription (ForeignKey to Transcription, optional)
  - content_type (enum: summary, notes, flashcards, quiz)
  - provider (enum: gemini, anthropic, openai, assemblyai_lemur)
  - content (JSONField)  # Structured content data
  - metadata (JSONField)  # Length, count, etc.
  - created_at (datetime)
  - processing_time_seconds (float)
  - cost_usd (decimal, optional)
```

### Task Model
```python
Task:
  - id (UUID, primary key)
  - user (ForeignKey to User)
  - task_type (enum: transcription, generation)
  - status (enum: pending, running, completed, failed, cancelled)
  - source (ForeignKey to Source)
  - parameters (JSONField)
  - result (JSONField, optional)
  - error_message (text, optional)
  - progress_percentage (integer)
  - created_at (datetime)
  - started_at (datetime, optional)
  - completed_at (datetime, optional)
```

### AnalyticsEvent Model
```python
AnalyticsEvent:
  - id (UUID, primary key)
  - user (ForeignKey to User, optional)
  - event_type (string, indexed)
  - event_data (JSONField)
  - timestamp (datetime, indexed)
  - session_id (string, optional)
```

## Technology Stack

### Backend
- **Python 3.11+**: Programming language
- **Django 4.2+**: Web framework
- **Django REST Framework**: API framework
- **Django Channels**: WebSocket support
- **Celery 5.3+**: Task queue
- **PostgreSQL 14+**: Database
- **Redis 7+**: Cache and message broker
- **boto3**: AWS S3/R2 client
- **SimpleJWT**: JWT authentication

### Frontend
- **React 18+**: UI framework
- **TypeScript**: Type safety
- **Vite**: Build tool
- **React Query**: Data fetching and caching
- **Socket.io-client**: WebSocket client
- **react-markdown**: Markdown rendering
- **jspdf**: PDF generation
- **Tailwind CSS**: Styling

### Infrastructure
- **Docker**: Containerization
- **Docker Compose**: Local development
- **Nginx**: Reverse proxy (production)
- **Gunicorn**: WSGI server (production)
- **Cloudflare R2**: Object storage (or AWS S3)

**Decision: Django + DRF for Backend**
- **Rationale**: Rapid development, built-in admin, strong ORM, excellent ecosystem, mature WebSocket support via Channels
- **Trade-offs**: Heavier than Flask/FastAPI, but provides more out-of-the-box features
- **Alternatives considered**: FastAPI (chose Django for admin and ORM), Flask (chose Django for structure)

**Decision: Celery for Background Processing**
- **Rationale**: Industry standard, excellent Django integration, supports complex workflows, reliable task execution
- **Trade-offs**: Requires message broker, but provides robust async processing
- **Alternatives considered**: Django-Q (chose Celery for maturity), RQ (chose Celery for features)

**Decision: Multiple LLM Provider Support**
- **Rationale**: Redundancy, cost optimization, provider-specific features, avoid vendor lock-in
- **Trade-offs**: More complex code, but provides flexibility and reliability
- **Alternatives considered**: Single provider (chose multi-provider for resilience)

**Decision: WebSocket for Real-time Updates**
- **Rationale**: Better UX than polling, efficient for long-running tasks, standard approach
- **Trade-offs**: More complex than polling, but significantly better user experience
- **Alternatives considered**: Polling (chose WebSocket for efficiency), Server-Sent Events (chose WebSocket for bidirectional)

**Decision: R2/S3-compatible Storage**
- **Rationale**: Cost-effective, scalable, reliable, standard API
- **Trade-offs**: External dependency, but necessary for file storage
- **Alternatives considered**: Local storage (chose cloud for scalability), Google Cloud Storage (chose R2/S3 for cost)

</architecture>

---

<risks>
<instruction>
Identify risks that could derail development and how to mitigate them.

Categories:
- Technical risks (complexity, unknowns)
- Dependency risks (blocking issues)
- Scope risks (creep, underestimation)
</instruction>

## Technical Risks

**Risk**: LLM Provider API Reliability and Rate Limits
- **Impact**: High - Core functionality depends on external APIs
- **Likelihood**: Medium - APIs can have outages or rate limits
- **Mitigation**: 
  - Implement multiple provider support with automatic fallback
  - Implement robust retry logic with exponential backoff
  - Monitor provider status and switch proactively
  - Cache responses where possible
- **Fallback**: 
  - Queue requests during outages
  - Notify users of delays
  - Implement request prioritization

**Risk**: Transcription Accuracy for Low-Quality Audio
- **Impact**: Medium - Affects content quality
- **Likelihood**: Medium - Users may upload poor quality files
- **Mitigation**: 
  - Validate audio quality before transcription
  - Provide audio enhancement options
  - Use providers with best accuracy for low-quality audio
  - Set user expectations about quality requirements
- **Fallback**: 
  - Allow manual transcription upload
  - Provide transcription editing interface

**Risk**: Long File Processing Times
- **Impact**: Medium - Poor user experience for long files
- **Likelihood**: High - Large files are common
- **Mitigation**: 
  - Implement chunking for very long files
  - Provide accurate progress estimates
  - Use WebSocket for real-time updates
  - Optimize provider selection for speed
- **Fallback**: 
  - Allow background processing with email notification
  - Implement file size limits with clear messaging

**Risk**: Cost Overruns from AI Provider Usage
- **Impact**: High - Can make product unprofitable
- **Likelihood**: Medium - Unpredictable usage patterns
- **Mitigation**: 
  - Implement cost tracking and alerts
  - Set usage limits per user
  - Optimize prompts to reduce token usage
  - Cache generated content when possible
  - Monitor costs in real-time
- **Fallback**: 
  - Implement tiered pricing
  - Rate limit heavy users
  - Switch to cheaper providers when possible

**Risk**: WebSocket Connection Stability
- **Impact**: Medium - Real-time updates may fail
- **Likelihood**: Medium - Network issues are common
- **Mitigation**: 
  - Implement automatic reconnection
  - Fallback to polling if WebSocket fails
  - Handle connection drops gracefully
  - Queue status updates during disconnection
- **Fallback**: 
  - Polling fallback mechanism
  - Email notifications for completion

## Dependency Risks

**Risk**: External API Changes Breaking Integration
- **Impact**: High - Core functionality breaks
- **Likelihood**: Low-Medium - APIs do change
- **Mitigation**: 
  - Pin API library versions
  - Monitor API changelogs
  - Implement versioned API clients
  - Comprehensive integration tests
- **Fallback**: 
  - Quick provider switching
  - Maintain multiple API versions
  - Rapid deployment pipeline

**Risk**: YouTube API Quota Limits
- **Impact**: Medium - YouTube ingestion may fail
- **Likelihood**: Medium - Quotas can be exceeded
- **Mitigation**: 
  - Monitor quota usage
  - Implement quota-aware request batching
  - Cache metadata when possible
  - Use alternative extraction methods (yt-dlp)
- **Fallback**: 
  - Switch to yt-dlp for direct download
  - Queue requests during quota exhaustion

**Risk**: Storage Provider Outage
- **Impact**: High - File uploads fail
- **Likelihood**: Low - But possible
- **Mitigation**: 
  - Use reliable storage provider (R2/S3)
  - Implement retry logic for storage operations
  - Monitor storage health
  - Implement storage redundancy
- **Fallback**: 
  - Temporary local storage during outages
  - Queue uploads for retry

## Scope Risks

**Risk**: Feature Creep Beyond v1 Scope
- **Impact**: Medium - Delays launch, increases complexity
- **Likelihood**: High - Many tempting features
- **Mitigation**: 
  - Strictly adhere to v1 feature list
  - Document future enhancements separately
  - Regular scope reviews
  - Clear definition of "done"
- **Fallback**: 
  - Defer non-essential features to v2
  - Prioritize core value proposition

**Risk**: Underestimating Content Generation Quality Requirements
- **Impact**: High - Users may not find output useful
- **Likigation**: Medium - Quality is subjective
- **Mitigation**: 
  - Early user testing with real content
  - Iterate on prompts based on feedback
  - A/B test different generation strategies
  - Set clear quality metrics
- **Fallback**: 
  - Allow extensive customization
  - Provide refinement tools
  - Support manual editing

**Risk**: Mobile Responsiveness Complexity
- **Impact**: Medium - Mobile users have poor experience
- **Likelihood**: Medium - Mobile adds complexity
- **Mitigation**: 
  - Design mobile-first
  - Test on real devices early
  - Use responsive frameworks (Tailwind)
  - Prioritize mobile UX
- **Fallback**: 
  - Launch desktop-first, iterate mobile
  - Provide mobile-optimized workflows

**Risk**: Integration Testing Complexity
- **Impact**: Medium - Bugs discovered late
- **Likelihood**: Medium - Many external dependencies
- **Mitigation**: 
  - Comprehensive mock strategy
  - Integration test environment
  - Test external integrations early
  - Monitor integration health
- **Fallback**: 
  - Manual testing procedures
  - Staged rollout
  - Feature flags for risky features

</risks>

---

<appendix>
## References

- Django REST Framework Documentation: https://www.django-rest-framework.org/
- Celery Documentation: https://docs.celeryproject.org/
- OpenAI Whisper API: https://platform.openai.com/docs/guides/speech-to-text
- AssemblyAI API Documentation: https://www.assemblyai.com/docs
- Google Gemini API: https://ai.google.dev/docs
- Anthropic Claude API: https://docs.anthropic.com/
- Django Channels: https://channels.readthedocs.io/
- Repository Planning Graph Method: Microsoft Research

## Glossary

- **Transcription**: Converting audio/video to text
- **Speaker Diarization**: Identifying and labeling different speakers in audio
- **LLM**: Large Language Model (e.g., GPT-4, Claude, Gemini)
- **LeMUR**: AssemblyAI's LLM for long-form content understanding
- **JWT**: JSON Web Token for authentication
- **WebSocket**: Bidirectional communication protocol for real-time updates
- **Celery**: Distributed task queue system
- **R2**: Cloudflare R2 object storage (S3-compatible)
- **DRF**: Django REST Framework
- **TDD**: Test-Driven Development
- **RPG**: Repository Planning Graph methodology

## Open Questions

1. **Cost Model**: Should we implement usage-based pricing or subscription tiers?
   - Decision needed: Before Phase 7 (Analytics)
   - Impact: Affects cost tracking requirements

2. **File Size Limits**: What are the maximum file sizes we'll support?
   - Decision needed: Before Phase 2 (Ingestion)
   - Impact: Affects storage and processing design

3. **Content Retention**: How long should we store generated content?
   - Decision needed: Before Phase 6 (UI)
   - Impact: Affects storage costs and cleanup policies

4. **Provider Selection Logic**: What criteria should determine provider selection?
   - Decision needed: Before Phase 3 (Transcription) and Phase 4 (Generation)
   - Impact: Affects provider selector implementation

5. **Mobile App**: Should v1 include mobile apps or web-only?
   - Decision needed: Before Phase 6 (UI)
   - Impact: Affects UI development scope

6. **Export Formats**: Beyond PDF/Markdown/HTML, what other formats are needed?
   - Decision needed: Before Phase 6 (UI)
   - Impact: Affects export implementation

7. **Rate Limiting**: What rate limits should we enforce per user?
   - Decision needed: Before Phase 7 (Analytics)
   - Impact: Affects cost control and user experience

8. **Error Recovery**: How should we handle partial failures (e.g., transcription succeeds but generation fails)?
   - Decision needed: Before Phase 5 (Background Processing)
   - Impact: Affects task orchestration design

</appendix>

---

<task-master-integration>
# How Task Master Uses This PRD

When you run `task-master parse-prd <file>.txt`, the parser:

1. **Extracts capabilities** → Main tasks
   - Each `### Capability:` becomes a top-level task

2. **Extracts features** → Subtasks
   - Each `#### Feature:` becomes a subtask under its capability

3. **Parses dependencies** → Task dependencies
   - `Depends on: [X, Y]` sets task.dependencies = ["X", "Y"]

4. **Orders by phases** → Task priorities
   - Phase 0 tasks = highest priority
   - Phase N tasks = lower priority, properly sequenced

5. **Uses test strategy** → Test generation context
   - Feeds test scenarios to Surgical Test Generator during implementation

**Result**: A dependency-aware task graph that can be executed in topological order.

## Why RPG Structure Matters

Traditional flat PRDs lead to:
- ❌ Unclear task dependencies
- ❌ Arbitrary task ordering
- ❌ Circular dependencies discovered late
- ❌ Poorly scoped tasks

RPG-structured PRDs provide:
- ✅ Explicit dependency chains
- ✅ Topological execution order
- ✅ Clear module boundaries
- ✅ Validated task graph before implementation

## Tips for Best Results

1. **Spend time on dependency graph** - This is the most valuable section for Task Master
2. **Keep features atomic** - Each feature should be independently testable
3. **Progressive refinement** - Start broad, use `task-master expand` to break down complex tasks
4. **Use research mode** - `task-master parse-prd --research` leverages AI for better task generation
</task-master-integration>

